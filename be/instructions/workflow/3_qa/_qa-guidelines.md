# QA 공통 가이드라인

## 테스트 자동화 전략

### 1. 단위 테스트 (Unit Test)
- **범위**: 각 서비스 클래스의 비즈니스 로직
- **도구**: JUnit 5, Mockito, AssertJ
- **커버리지 목표**: 90% 이상
- **실행 주기**: 코드 변경 시마다 실행 (CI/CD 파이프라인)

### 2. 통합 테스트 (Integration Test)
- **범위**: API 엔드포인트, 데이터베이스 연동
- **도구**: Spring Boot Test, TestContainers
- **테스트 데이터**: 각 테스트마다 독립적인 데이터셋
- **실행 주기**: PR 생성 시 자동 실행

### 3. 엔드투엔드 테스트 (E2E Test)
- **범위**: 전체 사용자 시나리오
- **도구**: REST Assured 또는 Selenium
- **테스트 환경**: 스테이징 환경에서 실행
- **실행 주기**: 배포 전 필수 실행

### 4. 성능 테스트 자동화
- **도구**: JMeter, K6
- **실행 주기**: 매일 밤 자동 실행
- **알림**: 성능 기준 미달 시 즉시 알림
- **기본 성능 기준**:
  - 평균 응답 시간: 200ms 이하 (조회 API)
  - 평균 응답 시간: 500ms 이하 (생성/수정 API)
  - 95 percentile: 1초 이하
  - 에러율: 1% 이하

### 5. 보안 테스트 자동화
- **도구**: OWASP ZAP, SonarQube
- **검증 항목**: SQL 인젝션, XSS, 인증/권한 우회
- **실행 주기**: 코드 변경 시마다 실행
- **보안 체크리스트**:
  - 인증/인가 검증
  - 입력값 검증
  - 에러 메시지 점검
  - 민감 정보 노출 방지

## 버그 리포팅 및 추적 체계

### 1. 버그 심각도 분류

#### Critical (심각)
- **정의**: 시스템 다운, 데이터 손실, 보안 취약점, 핵심 기능 완전 장애
- **대응 시간**: 즉시 (1시간 이내)
- **예시**: 
  - 전체 시스템 다운
  - 사용자 데이터 유실
  - 권한 우회 보안 이슈

#### High (높음)
- **정의**: 주요 기능 부분 장애, 성능 심각한 저하, 다수 사용자 영향
- **대응 시간**: 당일 내 (8시간 이내)
- **예시**:
  - 주요 API 응답 불가
  - 대량 사용자 로그인 실패
  - 성능 50% 이상 저하

#### Medium (보통)
- **정의**: 일부 기능 오동작, UI/UX 문제, 특정 조건에서만 발생
- **대응 시간**: 3일 이내
- **예시**:
  - 특정 브라우저에서 기능 오동작
  - UI 레이아웃 깨짐
  - 에러 메시지 부정확

#### Low (낮음)
- **정의**: 사소한 표시 오류, 개선 사항, 문서 오타
- **대응 시간**: 1주일 이내
- **예시**:
  - 텍스트 오타
  - 불필요한 로그 출력
  - 코드 최적화 여지

### 2. 버그 리포트 템플릿

```markdown
## 버그 정보
- **ID**: BUG-YYYY-NNNN
- **제목**: [간단한 버그 설명]
- **심각도**: Critical/High/Medium/Low
- **우선순위**: P0/P1/P2/P3
- **발견자**: [이름]
- **발견일**: YYYY-MM-DD
- **담당자**: [할당된 개발자]

## 환경 정보
- **OS**: 
- **브라우저**: 
- **버전**: 
- **테스트 환경**: Local/Dev/Staging/Production

## 재현 방법
1. [단계별 재현 방법]
2. 
3. 

## 예상 결과
[정상적인 동작 설명]

## 실제 결과
[실제 발생한 문제 설명]

## 추가 정보
- **스크린샷**: [첨부]
- **로그**: [관련 로그]
- **에러 메시지**: [정확한 에러 내용]
- **영향 범위**: [영향받는 사용자/기능 범위]
- **기타**: [추가 설명]

## 해결 방안 (개발자 작성)
- **원인 분석**: 
- **해결 방법**: 
- **테스트 방법**: 
- **배포 계획**: 
```

### 3. 버그 처리 프로세스

#### 3.1 버그 라이프사이클
```
발견 → 등록 → 분류 → 할당 → 수정 → 검증 → 완료
```

#### 3.2 상세 프로세스
1. **발견**: 테스터 또는 사용자가 버그 발견
2. **등록**: 버그 추적 시스템(Jira/GitHub Issues)에 등록
3. **분류**: 개발팀에서 심각도 및 우선순위 결정
4. **할당**: 담당 개발자 배정
5. **수정**: 개발자가 버그 수정 및 PR 생성
6. **검증**: 테스터가 수정 사항 확인
7. **완료**: 버그 해결 완료 처리

#### 3.3 에스컬레이션 규칙
- **Critical**: 1시간 내 미대응 시 팀 리드에게 에스컬레이션
- **High**: 당일 내 미대응 시 팀 리드에게 에스컬레이션
- **Medium/Low**: 예정 기한 초과 시 주간 회의에서 논의

### 4. 주요 메트릭

#### 4.1 품질 메트릭
- **버그 발견율**: 단위 시간당 발견되는 버그 수
- **버그 해결율**: 단위 시간당 해결되는 버그 수
- **버그 재발율**: 수정 후 재발하는 버그 비율
- **평균 해결 시간**: 버그 발견부터 해결까지 소요 시간
- **테스트 커버리지**: 코드 커버리지 비율

#### 4.2 성능 메트릭
- **응답 시간**: API별 평균/최대 응답 시간
- **처리량**: 단위 시간당 처리 가능한 요청 수
- **에러율**: 전체 요청 대비 에러 발생 비율
- **리소스 사용률**: CPU, 메모리, 디스크 사용률

#### 4.3 사용자 경험 메트릭
- **페이지 로드 시간**: 각 페이지별 로딩 시간
- **사용자 만족도**: 설문조사 결과
- **기능 사용률**: 각 기능별 사용 빈도

## 테스트 환경 구성

### 1. 환경별 역할
- **Local**: 개발자 개인 테스트
- **Development**: 통합 개발 테스트
- **Staging**: 배포 전 최종 검증
- **Production**: 실제 서비스 환경

### 2. 테스트 데이터 관리
- **테스트 데이터 격리**: 환경별 독립적인 데이터베이스
- **데이터 리셋**: 테스트 완료 후 자동 정리
- **시드 데이터**: 기본 테스트 시나리오용 데이터 제공

### 3. 모니터링 및 로깅
- **로그 레벨**: 환경별 적절한 로그 레벨 설정
- **메트릭 수집**: 응답 시간, 에러율, 리소스 사용률
- **알림 설정**: 임계치 초과 시 즉시 알림

## 품질 게이트

### 1. 코드 품질 기준
- **테스트 커버리지**: 90% 이상
- **정적 분석**: SonarQube 품질 게이트 통과
- **코드 리뷰**: 최소 2명 이상 승인

### 2. 성능 기준
- **API 응답 시간**: 기준치 이내
- **부하 테스트**: 목표 동시 사용자 수 처리 가능
- **메모리 누수**: 장시간 실행 시 메모리 증가 없음

### 3. 보안 기준
- **보안 스캔**: 취약점 0개
- **권한 검증**: 모든 API 엔드포인트 권한 검증 완료
- **데이터 검증**: 입력값 검증 로직 구현

## 회귀 테스트 전략

### 1. 회귀 테스트 범위
- **전체 회귀**: 메이저 릴리스 전
- **부분 회귀**: 마이너 변경 시 관련 모듈만
- **스모크 테스트**: 배포 후 핵심 기능 동작 확인

### 2. 자동화 우선순위
1. **P0**: 핵심 비즈니스 로직
2. **P1**: 주요 사용자 시나리오
3. **P2**: 일반적인 기능
4. **P3**: 예외 상황 처리

### 3. 실행 주기
- **코드 변경 시**: 단위 테스트 + 관련 통합 테스트
- **일일**: 전체 회귀 테스트 스위트
- **주간**: 성능 테스트 + 보안 테스트
- **릴리스 전**: 전체 테스트 + 수동 테스트
